{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Workspace-set-up\" data-toc-modified-id=\"Workspace-set-up-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Workspace set-up</a></div><div class=\"lev1 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev2 toc-item\"><a href=\"#What-is-this-tutorial-for?\" data-toc-modified-id=\"What-is-this-tutorial-for?-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>What is this tutorial for?</a></div><div class=\"lev2 toc-item\"><a href=\"#Background\" data-toc-modified-id=\"Background-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Background</a></div><div class=\"lev3 toc-item\"><a href=\"#Notation\" data-toc-modified-id=\"Notation-221\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Notation</a></div><div class=\"lev3 toc-item\"><a href=\"#Tools-from-probability\" data-toc-modified-id=\"Tools-from-probability-222\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Tools from probability</a></div><div class=\"lev1 toc-item\"><a href=\"#Matrix-completion-theory\" data-toc-modified-id=\"Matrix-completion-theory-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Matrix completion theory</a></div><div class=\"lev2 toc-item\"><a href=\"#First-pass-at-the-problem\" data-toc-modified-id=\"First-pass-at-the-problem-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>First pass at the problem</a></div><div class=\"lev2 toc-item\"><a href=\"#Motivation-for-solving-this-problem\" data-toc-modified-id=\"Motivation-for-solving-this-problem-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Motivation for solving this problem</a></div><div class=\"lev2 toc-item\"><a href=\"#Making-the-problem-feasible\" data-toc-modified-id=\"Making-the-problem-feasible-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Making the problem feasible</a></div><div class=\"lev3 toc-item\"><a href=\"#&quot;Entry-determinism&quot;\" data-toc-modified-id=\"&quot;Entry-determinism&quot;-331\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>\"Entry determinism\"</a></div><div class=\"lev4 toc-item\"><a href=\"#Example:-Singular-Value-Decomposition\" data-toc-modified-id=\"Example:-Singular-Value-Decomposition-3311\"><span class=\"toc-item-num\">3.3.1.1&nbsp;&nbsp;</span>Example: Singular Value Decomposition</a></div><div class=\"lev3 toc-item\"><a href=\"#Sampling-pattern\" data-toc-modified-id=\"Sampling-pattern-332\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Sampling pattern</a></div><div class=\"lev3 toc-item\"><a href=\"#&quot;Spiky-matrices&quot;\" data-toc-modified-id=\"&quot;Spiky-matrices&quot;-333\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>\"Spiky matrices\"</a></div><div class=\"lev2 toc-item\"><a href=\"#A-convex-program-for-incoherent-low-rank-matrix-completion\" data-toc-modified-id=\"A-convex-program-for-incoherent-low-rank-matrix-completion-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>A convex program for incoherent low-rank matrix completion</a></div><div class=\"lev3 toc-item\"><a href=\"#Set-up\" data-toc-modified-id=\"Set-up-341\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Set-up</a></div><div class=\"lev3 toc-item\"><a href=\"#Recovery-guarantees\" data-toc-modified-id=\"Recovery-guarantees-342\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Recovery guarantees</a></div><div class=\"lev3 toc-item\"><a href=\"#Noisy-recovery\" data-toc-modified-id=\"Noisy-recovery-343\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Noisy recovery</a></div><div class=\"lev1 toc-item\"><a href=\"#Algorithms-for-matrix-completion\" data-toc-modified-id=\"Algorithms-for-matrix-completion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Algorithms for matrix completion</a></div><div class=\"lev2 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev3 toc-item\"><a href=\"#Notation\" data-toc-modified-id=\"Notation-411\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Notation</a></div><div class=\"lev2 toc-item\"><a href=\"#Nuclear-norm-minimization\" data-toc-modified-id=\"Nuclear-norm-minimization-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Nuclear norm minimization</a></div><div class=\"lev3 toc-item\"><a href=\"#An-alternative-format\" data-toc-modified-id=\"An-alternative-format-421\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>An alternative format</a></div><div class=\"lev2 toc-item\"><a href=\"#Alternating-direction-method-(ADM)\" data-toc-modified-id=\"Alternating-direction-method-(ADM)-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Alternating-direction method (ADM)</a></div><div class=\"lev3 toc-item\"><a href=\"#Frobenius-norm-minimization\" data-toc-modified-id=\"Frobenius-norm-minimization-431\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Frobenius norm minimization</a></div><div class=\"lev3 toc-item\"><a href=\"#Exercises:-write-an-algorithm\" data-toc-modified-id=\"Exercises:-write-an-algorithm-432\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Exercises: write an algorithm</a></div><div class=\"lev2 toc-item\"><a href=\"#Debiasing-low-rank-projection-method\" data-toc-modified-id=\"Debiasing-low-rank-projection-method-44\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Debiasing low-rank projection method</a></div><div class=\"lev3 toc-item\"><a href=\"#Overview\" data-toc-modified-id=\"Overview-441\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Overview</a></div><div class=\"lev3 toc-item\"><a href=\"#Algorithm\" data-toc-modified-id=\"Algorithm-442\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Algorithm</a></div><div class=\"lev3 toc-item\"><a href=\"#Results\" data-toc-modified-id=\"Results-443\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>Results</a></div><div class=\"lev3 toc-item\"><a href=\"#Exercise:-Code-the-algorithm\" data-toc-modified-id=\"Exercise:-Code-the-algorithm-444\"><span class=\"toc-item-num\">4.4.4&nbsp;&nbsp;</span>Exercise: Code the algorithm</a></div><div class=\"lev2 toc-item\"><a href=\"#Max-norm-minimization\" data-toc-modified-id=\"Max-norm-minimization-45\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Max-norm minimization</a></div><div class=\"lev3 toc-item\"><a href=\"#Exercise:-write-an-algorithm\" data-toc-modified-id=\"Exercise:-write-an-algorithm-451\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Exercise: write an algorithm</a></div><div class=\"lev2 toc-item\"><a href=\"#On-selecting-the-rank\" data-toc-modified-id=\"On-selecting-the-rank-46\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>On selecting the rank</a></div><div class=\"lev3 toc-item\"><a href=\"#Issues\" data-toc-modified-id=\"Issues-461\"><span class=\"toc-item-num\">4.6.1&nbsp;&nbsp;</span>Issues</a></div><div class=\"lev3 toc-item\"><a href=\"#Exercises:-making-progress-on-the-issues-above\" data-toc-modified-id=\"Exercises:-making-progress-on-the-issues-above-462\"><span class=\"toc-item-num\">4.6.2&nbsp;&nbsp;</span>Exercises: making progress on the issues above</a></div><div class=\"lev1 toc-item\"><a href=\"#Coding-matrix-completion-problems\" data-toc-modified-id=\"Coding-matrix-completion-problems-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Coding matrix completion problems</a></div><div class=\"lev2 toc-item\"><a href=\"#Nuclear-norm-minimization-for-matrix-completion\" data-toc-modified-id=\"Nuclear-norm-minimization-for-matrix-completion-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Nuclear norm minimization for matrix completion</a></div><div class=\"lev3 toc-item\"><a href=\"#Description\" data-toc-modified-id=\"Description-511\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Description</a></div><div class=\"lev3 toc-item\"><a href=\"#Problem-set-up\" data-toc-modified-id=\"Problem-set-up-512\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Problem set-up</a></div><div class=\"lev2 toc-item\"><a href=\"#Implement-an-alternating-direction-method\" data-toc-modified-id=\"Implement-an-alternating-direction-method-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Implement an alternating-direction method</a></div><div class=\"lev3 toc-item\"><a href=\"#Exercises:-Implement-the-ADM-algorithm-for-Frobenius-norm-minimization\" data-toc-modified-id=\"Exercises:-Implement-the-ADM-algorithm-for-Frobenius-norm-minimization-521\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Exercises: Implement the ADM algorithm for Frobenius norm minimization</a></div><div class=\"lev2 toc-item\"><a href=\"#Debiasing-low-rank-projection-implementation\" data-toc-modified-id=\"Debiasing-low-rank-projection-implementation-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Debiasing low-rank projection implementation</a></div><div class=\"lev3 toc-item\"><a href=\"#Approximate-observation-mask\" data-toc-modified-id=\"Approximate-observation-mask-531\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Approximate observation mask</a></div><div class=\"lev4 toc-item\"><a href=\"#Method-1:-svds\" data-toc-modified-id=\"Method-1:-svds-5311\"><span class=\"toc-item-num\">5.3.1.1&nbsp;&nbsp;</span>Method 1: <code>svds</code></a></div><div class=\"lev4 toc-item\"><a href=\"#Method-2:-altMinSense\" data-toc-modified-id=\"Method-2:-altMinSense-5312\"><span class=\"toc-item-num\">5.3.1.2&nbsp;&nbsp;</span>Method 2: <code>altMinSense</code></a></div><div class=\"lev4 toc-item\"><a href=\"#Comparison\" data-toc-modified-id=\"Comparison-5313\"><span class=\"toc-item-num\">5.3.1.3&nbsp;&nbsp;</span>Comparison</a></div><div class=\"lev3 toc-item\"><a href=\"#Compute-hat-M_0\" data-toc-modified-id=\"Compute-hat-M_0-532\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Compute $\\hat M_0$</a></div><div class=\"lev3 toc-item\"><a href=\"#Take-the-Hadamard-product\" data-toc-modified-id=\"Take-the-Hadamard-product-533\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Take the Hadamard product</a></div><div class=\"lev3 toc-item\"><a href=\"#Observe-the-results\" data-toc-modified-id=\"Observe-the-results-534\"><span class=\"toc-item-num\">5.3.4&nbsp;&nbsp;</span>Observe the results</a></div><div class=\"lev1 toc-item\"><a href=\"#On-the-use-of-sparse-matrices\" data-toc-modified-id=\"On-the-use-of-sparse-matrices-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>On the use of sparse matrices</a></div><div class=\"lev1 toc-item\"><a href=\"#The-realm-of-parameter-guessing\" data-toc-modified-id=\"The-realm-of-parameter-guessing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>The realm of parameter guessing</a></div><div class=\"lev2 toc-item\"><a href=\"#What-happens-when-rank-is-chosen-incorrectly?\" data-toc-modified-id=\"What-happens-when-rank-is-chosen-incorrectly?-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>What happens when rank is chosen incorrectly?</a></div><div class=\"lev2 toc-item\"><a href=\"#What-happens-in-the-presence-of-noise?\" data-toc-modified-id=\"What-happens-in-the-presence-of-noise?-72\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>What happens in the presence of noise?</a></div><div class=\"lev1 toc-item\"><a href=\"#Out-of-the-box-packages\" data-toc-modified-id=\"Out-of-the-box-packages-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Out-of-the-box packages</a></div><div class=\"lev1 toc-item\"><a href=\"#Example:-Seismic-Data\" data-toc-modified-id=\"Example:-Seismic-Data-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Example: Seismic Data</a></div><div class=\"lev2 toc-item\"><a href=\"#seismic-data-interpolation-via-matrix-completion\" data-toc-modified-id=\"seismic-data-interpolation-via-matrix-completion-91\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Seismic Data Interpolation via Matrix Completion</a></div><div class=\"lev3 toc-item\"><a href=\"#seismic-data-acquisition\" data-toc-modified-id=\"seismic-data-acquisition-911\"><span class=\"toc-item-num\">9.1.1&nbsp;&nbsp;</span>Seismic Data Acquisition</a></div><div class=\"lev3 toc-item\"><a href=\"#low-rank-data-structure\" data-toc-modified-id=\"low-rank-data-structure-912\"><span class=\"toc-item-num\">9.1.2&nbsp;&nbsp;</span>Low-Rank Data Structure</a></div><div class=\"lev3 toc-item\"><a href=\"#interpolation-example\" data-toc-modified-id=\"interpolation-example-913\"><span class=\"toc-item-num\">9.1.3&nbsp;&nbsp;</span>Interpolation Example</a></div><div class=\"lev3 toc-item\"><a href=\"#Exercises:-visualizing-the-input-data\" data-toc-modified-id=\"Exercises:-visualizing-the-input-data-914\"><span class=\"toc-item-num\">9.1.4&nbsp;&nbsp;</span>Exercises: visualizing the input data</a></div><div class=\"lev3 toc-item\"><a href=\"#Exercise:-Setting-up-the-problem\" data-toc-modified-id=\"Exercise:-Setting-up-the-problem-915\"><span class=\"toc-item-num\">9.1.5&nbsp;&nbsp;</span>Exercise: Setting up the problem</a></div><div class=\"lev3 toc-item\"><a href=\"#Visualize-the-seismic-observations\" data-toc-modified-id=\"Visualize-the-seismic-observations-916\"><span class=\"toc-item-num\">9.1.6&nbsp;&nbsp;</span>Visualize the seismic observations</a></div><div class=\"lev3 toc-item\"><a href=\"#Exercise:-Recover-the-array\" data-toc-modified-id=\"Exercise:-Recover-the-array-917\"><span class=\"toc-item-num\">9.1.7&nbsp;&nbsp;</span>Exercise: Recover the array</a></div><div class=\"lev1 toc-item\"><a href=\"#Example:-Movie-ratings\" data-toc-modified-id=\"Example:-Movie-ratings-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Example: Movie ratings</a></div><div class=\"lev2 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-101\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev3 toc-item\"><a href=\"#About-the-data\" data-toc-modified-id=\"About-the-data-1011\"><span class=\"toc-item-num\">10.1.1&nbsp;&nbsp;</span>About the data</a></div><div class=\"lev1 toc-item\"><a href=\"#summary\" data-toc-modified-id=\"summary-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Summary</a></div><div class=\"lev1 toc-item\"><a href=\"#usage-license\" data-toc-modified-id=\"usage-license-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Usage License</a></div><div class=\"lev1 toc-item\"><a href=\"#citation\" data-toc-modified-id=\"citation-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Citation</a></div><div class=\"lev1 toc-item\"><a href=\"#further-information-about-grouplens\" data-toc-modified-id=\"further-information-about-grouplens-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Further Information About GroupLens</a></div><div class=\"lev1 toc-item\"><a href=\"#content-and-use-of-files\" data-toc-modified-id=\"content-and-use-of-files-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Content and Use of Files</a></div><div class=\"lev2 toc-item\"><a href=\"#formatting-and-encoding\" data-toc-modified-id=\"formatting-and-encoding-151\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;</span>Formatting and Encoding</a></div><div class=\"lev2 toc-item\"><a href=\"#user-ids\" data-toc-modified-id=\"user-ids-152\"><span class=\"toc-item-num\">15.2&nbsp;&nbsp;</span>User Ids</a></div><div class=\"lev2 toc-item\"><a href=\"#movie-ids\" data-toc-modified-id=\"movie-ids-153\"><span class=\"toc-item-num\">15.3&nbsp;&nbsp;</span>Movie Ids</a></div><div class=\"lev2 toc-item\"><a href=\"#ratings-data-file-structure-ratings-csv-\" data-toc-modified-id=\"ratings-data-file-structure-ratings-csv--154\"><span class=\"toc-item-num\">15.4&nbsp;&nbsp;</span>Ratings Data File Structure (ratings.csv)</a></div><div class=\"lev2 toc-item\"><a href=\"#tags-data-file-structure-tags-csv-\" data-toc-modified-id=\"tags-data-file-structure-tags-csv--155\"><span class=\"toc-item-num\">15.5&nbsp;&nbsp;</span>Tags Data File Structure (tags.csv)</a></div><div class=\"lev2 toc-item\"><a href=\"#movies-data-file-structure-movies-csv-\" data-toc-modified-id=\"movies-data-file-structure-movies-csv--156\"><span class=\"toc-item-num\">15.6&nbsp;&nbsp;</span>Movies Data File Structure (movies.csv)</a></div><div class=\"lev2 toc-item\"><a href=\"#links-data-file-structure-links-csv-\" data-toc-modified-id=\"links-data-file-structure-links-csv--157\"><span class=\"toc-item-num\">15.7&nbsp;&nbsp;</span>Links Data File Structure (links.csv)</a></div><div class=\"lev2 toc-item\"><a href=\"#cross-validation\" data-toc-modified-id=\"cross-validation-158\"><span class=\"toc-item-num\">15.8&nbsp;&nbsp;</span>Cross-Validation</a></div><div class=\"lev2 toc-item\"><a href=\"#Exercise:-Complete-the-movies!\" data-toc-modified-id=\"Exercise:-Complete-the-movies!-159\"><span class=\"toc-item-num\">15.9&nbsp;&nbsp;</span>Exercise: Complete the movies!</a></div><div class=\"lev1 toc-item\"><a href=\"#Example:-Music-preference-data\" data-toc-modified-id=\"Example:-Music-preference-data-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Example: Music preference data</a></div><div class=\"lev1 toc-item\"><a href=\"#Further-Reading\" data-toc-modified-id=\"Further-Reading-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>Further Reading</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "from scipy.optimize import least_squares\n",
    "import scipy as sp\n",
    "import scipy.sparse as sparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from cvxpy import Variable, Minimize, Problem\n",
    "from cvxpy import norm as cvxnorm\n",
    "from cvxpy import mul_elemwise, SCS\n",
    "from cvxpy import vec as cvxvec\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "from mc_util import *\n",
    "from mc_solve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Notebook by [Aaron Berk](http://asberk.ca) for the 2017 [BC Data Science workshop](http://workshop.bcdata.ca). \n",
    "\n",
    "A special thanks to Yaniv Plan and Oscar Lopez for help in assembling code, data and materials for this tutorial. \n",
    "\n",
    "## What is this tutorial for?\n",
    "\n",
    "This tutorial is a whirlwind tour through matrix completion, designed for a one hour tutorial for the [BC Data Science Workshop](http://workshop.bcdata.ca) hosted at UBC in 2017. The first hour of content will introduce context and necessary tools for understanding the matrix completion problem, as well as some introductory code for solving the matrix completion problem. The remainder of the content will be devoted to the problem solving section of this mini-project, where participants will write code to extend introductory algorithms, and use their algorithms on real data.\n",
    "\n",
    "A [gist of this notebook](https://gist.github.com/asberk/8c8fd1e7f384df0183b8f01423c07a55) is available.\n",
    "\n",
    "## Background\n",
    "\n",
    "\n",
    "### Notation\n",
    "\n",
    "In this tutorial, we denote a fixed matrix $M \\in \\mathbb{R}^{m \\times n}$ where $m$ and $n$ are positive integers. Elements of $M$ will be denoted using subscript notation, so that the element in the $i$th row and $j$th column of $M$ is denoted by $M_{ij}$ for any $(i,j) \\in [m] \\times [n]$ where $[m] := \\{1, 2, \\ldots, m\\}$. We will say that a matrix $X$ is a **random matrix** if each of its entries $X_{ij}$ come from a distribution $\\mathcal{D}_{ij}$ (and write $X_{ij} \\sim \\mathcal{D}_{ij}$). Typically, one has $\\mathcal{D}_{ij} = \\mathcal{D}$ for some distribution $\\mathcal{D}$ and all $i,j$ (*e.g.*, the standard normal distribution $\\mathcal{D} \\sim \\mathcal{N}(0,1)$). Moreover, for the purpose of this tutorial, we will assume that $X_{ij} \\overset{iid}{\\sim} \\mathcal{D}$ are **independent and identically distributed** (iid) samples from the distribution $\\mathcal{D}$. In general, this may be much too strict an assumption; we will explore this in some detail below. \n",
    "\n",
    "### Tools from probability\n",
    "\n",
    "For $\\Omega \\subseteq [m] \\times [n]$, a subset of indices of a matrix, the **subsampled matrix** $M_\\Omega$ has entries given by \n",
    "$$\n",
    "M_\\Omega := \\begin{cases}\n",
    "M_{ij} & (i,j) \\in \\Omega\\\\\n",
    "0 & (i,j) \\not\\in \\Omega\n",
    "\\end{cases}\n",
    "$$\n",
    "Entries of a fixed matrix $M$ are observed **uniform at random** with probability $p \\in (0,1)$ if, for all $(i,j) \\in [m]\\times[n]$, the probability that $(i,j) \\in \\Omega$ is equal to $p$. \n",
    "\n",
    "We denote by $\\mathbb E$ the expectation operator for a random variable; and $\\mathrm{Var}$ the variance. Let $\\hat \\theta$ be an estimator for a random variable $\\theta$. The **bias** of the estimator $\\hat \\theta$ is given by $B(\\hat\\theta) = \\mathbb E [\\hat \\theta] - \\theta$. An estimator for which $B \\equiv 0$ is said to be an **unbiased estimator**.\n",
    "\n",
    "# Matrix completion theory\n",
    "\n",
    "## First pass at the problem\n",
    "\n",
    "Let $M \\in \\mathbb{R}^{n\\times n}$ be a fixed real-valued square matrix and let $\\Omega \\subseteq [n] \\times [n]$ be a set of indices of the matrix corresponding to the set of observed entries. \n",
    "\n",
    "A first pass at the **matrix completion problem** is as follows:\n",
    "\n",
    "> Given only $\\Omega\\subseteq [n]\\times[n]$ and corresponding entries $\\{M_{ij}\\}_{(i,j) \\in \\Omega}$, recover the full matrix $M$.\n",
    "\n",
    "## Motivation for solving this problem\n",
    "\n",
    "A good question to ask in an applied math workshop: besides mathematical interest, why do we care about solving this problem? Below are a few examples, with asterisks beside ones that make guest appearances in this tutorial.\n",
    "\n",
    "* The Netflix challenge \\*\n",
    "* Subsurface reconstruction in geophysics and petrology\n",
    "* Bathymetry interpolation in meteorology, hydrology and geophysics \\*\n",
    "* *Terroir* characterization\n",
    "* Online shopping patterns\n",
    "* Music listening & book reading preferences \\*\n",
    "* Inference on certain graph structures \\*\n",
    "\n",
    "## Making the problem feasible\n",
    "\n",
    "### \"Entry determinism\"\n",
    "\n",
    "As it is posed above, this problem is unfeasible. **Why!?** In general, one element in a matrix **does not determine** the other elements in a matrix! In order to make the matrix completion problem tractable, we have to impose some constraints on our matrix — these constraints will ensure that [in some sense], the entries we have observed will determine the values that we have not observed.\n",
    "\n",
    "**So what should this constraint be?** Just as when working with vectors, there could be several ways to constrain a given vector; however, a common theme that runs between all of the examples listed above is that the associated matrices have some **intrinsic low-dimensional structure**. <div class='alert alert-block alert-info'>For this reason, the relevant kind of structure with which we will imbue $M$ is a **constraint on the rank of $M$**. In particular, for a positive integer $r \\ll n$, we will assume that $\\mathrm{rank}(M) = r$. </div>\n",
    "\n",
    "With $M$ being *of low rank*, it follows that we can represent $M$ by \n",
    "$$\n",
    "M = WH^*, \\qquad W, H \\in \\mathbb{R}^{n\\times r}\n",
    "$$\n",
    "which is seen clearly from the SVD-like representation of $M$\n",
    "$$\n",
    "M = U \\Sigma V^* \\qquad U, V \\in \\mathbb{R}^{n\\times r}, \\Sigma \\in \\mathbb{R}^{r\\times r}\n",
    "$$\n",
    "where $\\Sigma$ is a diagonal matrix whose elements $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r$ are the **non-zero singular values** of $M$; and $U$ and $V$ are unitary matrices (*i.e.*, $UU^* \\equiv U^* U \\equiv VV^* \\equiv V^* V \\equiv 1$).\n",
    "\n",
    "#### Example: Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "n, r = (50, 5)\n",
    "W = np.random.randn(n,r)\n",
    "H = np.random.randn(n,r)\n",
    "M = W @ H.T\n",
    "\n",
    "# svd\n",
    "U, Sigma, Vtr = svd(M)\n",
    "\n",
    "print(U.shape)\n",
    "print(Sigma.size)\n",
    "print(Vtr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we really getting 50 singular values for `M`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Sigma[:2*r].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, but `n-r` of them are equal to $0$ (up to numerical precision). This means we can get a more efficient representation of `M` in SVD format. We'll take only the first `r` columns of `U`; the first `r` rows of `Vtr`; and only the positive singular values of `Sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U_r = U[:, :r]\n",
    "Vtr_r = Vtr[:r, :]\n",
    "Sigma_r = Sigma[:r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M2 = U_r @ np.diag(Sigma_r) @ Vtr_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('root mean-squared error = {:5.3g}'.format(rmse(M2, M)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `scipy.linalg.svd` computes the entire SVD of the matrix — if we know that a matrix is of low rank, then we can use the more efficient `randomized_svd` from `sklearn.decomposition.truncated_svd`, which returns only the top $k$ (or bottom $k$) singular vectors, where $k$ is an argument to the function. \n",
    "\n",
    "### Sampling pattern\n",
    "\n",
    "The last problem concerns which entries of $M$ we get to observe. For example, can the entries of $M$ be observed deterministically? Randomly (if so, uniform or non-uniform)? \n",
    "In an ideal scenario, we'd like the entries to be observed uniformly at random. Not only does this make the math easier, but it gives us good recovery results. We'll illustrate below what can go wrong if entries are observed non-uniformly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimension and rank\n",
    "m = n = 100\n",
    "r = 10\n",
    "# construct low rank components of M\n",
    "W = np.random.randn(m, r)\n",
    "H = np.random.randn(n, r)\n",
    "# introduce a \"bizarre structure\"\n",
    "W[:20,:] = 1\n",
    "H[-20:,:] = .75\n",
    "# compute M\n",
    "M = W @ H.T\n",
    "\n",
    "# get a non-uniform collection of indices\n",
    "i_nu, j_nu = aNonuniformSampling(m, n, .1)\n",
    "# Include a uniform sampling of indices for comparison\n",
    "i_u, j_u = np.where(np.random.rand(*M.shape) < .1)\n",
    "\n",
    "# Construct the subsampled matrices\n",
    "M_nu = np.zeros(M.shape)\n",
    "M_nu[i_nu, j_nu] = M[i_nu, j_nu]\n",
    "\n",
    "M_u = np.zeros(M.shape)\n",
    "M_u[i_u, j_u] = M[i_u, j_u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "plot_titles = ['$M$', '$M_\\mathrm{u}$', '$M_\\mathrm{nu}$']\n",
    "plt.figure(figsize=(16,4))\n",
    "for ctr, mat, ptitle in zip(range(3), [M, M_u, M_nu], plot_titles):\n",
    "    plt.subplot(1,3,ctr+1)\n",
    "    plt.imshow(mat, cmap='gray')\n",
    "    plt.title(ptitle,size=16)\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the particularly non-uniform sampling pattern, we cannot observe the behaviour in the upper right hand corner of $M$ — in general this kind of non-uniformity can be detrimental to recovery. However, it is not always practical or realistic to assume a uniform sampling of observations, so we shall look at one way of tackling non-uniform sampling patterns below. In the meantime, \n",
    "<div class='alert alert-block alert-info'>\n",
    "We will make the added assumption that the entries belonging to $\\Omega$ were observed **uniformly at random**.\n",
    "</div>\n",
    "\n",
    "By making this assumption, we gain our second small advantage. Namely, let $M \\in \\mathbb{R}^{n\\times n}$ be a square matrix of rank $r \\ll n$ and let $\\Omega \\subseteq [m] \\times [n]$ be a set of indices such that $(i,j) \\in \\Omega$ uniformly at random with probability $p \\in (0,1)$. Then the matrix\n",
    "$$\n",
    "\\frac{1}{p} M_\\Omega\n",
    "$$\n",
    "is an **unbiased estimator** for $M$. Namely, \n",
    "$$\n",
    "\\mathbb{E} \\big[ \\frac{1}{p} M_\\Omega \\big] = \\frac{p}{p} M + \\frac{1-p}{p} \\mathbf{0} = M\n",
    "$$\n",
    "\n",
    "### \"Spiky matrices\"\n",
    "\n",
    "A problem that could arise — particularly, as a result of the process determining which entries we get to observe — is if our matrix $M$ is **too spiky**. Probably the best way to illustrate spikiness is with an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.zeros((m,1))\n",
    "H = np.zeros((n,1))\n",
    "W[np.random.randint(n, size=5), 0] = 10*np.random.rand(5)\n",
    "H[np.random.randint(n, size=5), 0] = 10*np.random.rand(5)\n",
    "M_spiky = W @ H.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(M_spiky, cmap='gray')\n",
    "plt.title('A spiky matrix $M$', size=16)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $M$ is too spiky, then it is likely that — regardless of the type of random sampling we'll use — the entries of $M$ that we observe will \"miss\" the \"content\" of $M$ and give us a false idea of what $M$ looks like. In other words, it is likely to be the case that the content of $M$ we most want to observe in a \"spiky\" matrix is **where the spikes are**. However, because these spikes are sparse, it is also likely to be the case that we will fail to observe them when we sample entries of $M$!\n",
    "\n",
    "In other words, we will need $M$ to be **incoherent** in order for a random sampling scheme to work well [most of the time]. This means that the resulting entries of $M$ are a \"well-mixed\" combination of the entries of $W$ and $H$. To make this precise, let $X \\in \\mathbb{R}^{m\\times n}$ be a matrix such that the columns, $X_j \\in \\mathbb{R}^m$, of $X$ are normalized: $X_j^* X_j = 1$. The **coherence of the matrix X** is defined by\n",
    "$$\n",
    "\\mu(X) := \\max_{1 \\leq j \\neq k \\leq n} |\\langle X_j, X_k \\rangle|\n",
    "$$\n",
    "\n",
    "\n",
    "<div class='alert alert-info alert-block'>\n",
    "\n",
    "**Coherence facts**  \n",
    "\n",
    "1. Characterizes the dependence between columns of $X$  \n",
    "2. If $X$ is Unitary, then $\\mu(X) = 0$  \n",
    "3. If $n > m$ (like in Compressed Sensing), then $\\mu(X) > 0$. Specifically:  \n",
    "$\n",
    "\\qquad\\mu(X) \\geq \\sqrt{\\frac{n - m}{m (n - 1)}}\n",
    "$  \n",
    "4. **We want $\\mu(X)$ to be small** [so that it is \"more like a unitary matrix\"]\n",
    "\n",
    "</div>\n",
    "\n",
    "## A convex program for incoherent low-rank matrix completion\n",
    "\n",
    "### Set-up\n",
    "\n",
    "Supposing we were handed observations $M_\\Omega$ and knowledge about the rank $r$ of our matrix, $M$, the problem of recovering $M$ can be phrased suitably in the language of convex optimization:\n",
    "$$\n",
    "M_{\\#} := \\underset{X_\\Omega = M_\\Omega}{\\arg\\min}\\,\\, \\mathrm{rank}(X)\n",
    "$$\n",
    "*i.e.,* return the matrix of smallest rank that matches $M$ on the observation set $\\Omega$. Mild assumptions on the size of $p$ guarantee recovery, $M_{\\#} = M$ with high probability. \n",
    "\n",
    "Unfortunately, not only is the above problem not convex, but it's NP-hard to find the minimizer! Instead, we can pose the *convex relaxation* of the above problem using a surrogate objective function. This yields the new minimization problem\n",
    "$$\n",
    "M^* := \\underset{X_\\Omega = M_\\Omega}{\\arg\\min}\\,\\, \\|X\\|_*\n",
    "$$\n",
    "where $\\| \\cdot \\|_*$ denotes the nuclear norm, equal to the sum of the singular values of its argument. For example, using the SVD of $M$, where $\\Sigma = \\mathrm{diag}((\\sigma_i)_{i=1}^r)$, we have\n",
    "$$\n",
    "\\|M\\|_* = \\sum_{i=1}^n \\sigma_{i}\n",
    "$$\n",
    "\n",
    "### Recovery guarantees\n",
    "\n",
    "As the nuclear norm is a norm, it is convex and hence the *convex relaxation* yields a convex optimization problem. Again, under mild assumptions on $p$, one obtains with high probability the recovery of the original matrix, $M^* = M$. In particular, $|\\Omega| \\sim \\mathcal{O}(r \\max\\{m,n\\} \\mathrm{polylog}(\\max\\{m, n\\}))$ uniform-at-random observations of an incoherent matrix are sufficient to guarantee recovery of $M$ with exceedingly high probability. Given that $r \\ll \\min\\{m,n\\}$, this is typically a significant reduction in the number of measurements needed if $M$ were of full rank. \n",
    "\n",
    "### Noisy recovery\n",
    "\n",
    "Until this point we have only concerned ourselves with recovery of the matrix $M$; however, problems like subsurface reconstruction are *never* noise-free. We note briefly that there exist analogues of MC algorithms for noisy data, and that there exist convex programs for matrix completion that are robust to noise. For example, the noise-amenable analogue to nuclear norm minimization is as follows. For a noisy matrix $A = M + Z$ where $M$ has rank $r$ and [for example] $Z_{ij} \\sim \\mathcal{N}(0,\\eta^2)$, one can approximate $M$ from a noisy observation set $A_\\Omega$ by \n",
    "$$\n",
    "M^* := \\arg\\min \\|X\\|_* \\quad \\text{subj. to} \\quad \\|A - X\\|_F \\leq \\eta\n",
    "$$\n",
    "where $\\|\\cdot \\|_F$ is the Frobenius norm for matrices, defined by\n",
    "$$\n",
    "\\|A\\|_F^2 = \\sum_{i,j} |A_{ij}|^2.\n",
    "$$\n",
    "\n",
    "# Algorithms for matrix completion\n",
    "\n",
    "## Introduction\n",
    "\n",
    "One could argue that there are four main types of algorithms for matrix completion.\n",
    "\n",
    "1. Nuclear norm minimization\n",
    "2. Alternating direction method\n",
    "3. Debiasing low-rank projection\n",
    "4. Max-norm minimization\n",
    "\n",
    "Each type holds particular advantages over the other types: some have better developed theory, while others have better applicability or efficacy in practice, while still others are have greater ease of implementation.\n",
    "\n",
    "### Notation\n",
    "\n",
    "We define the additional notation $P_\\Omega$ to denote the sampling/projection operator,\n",
    "$$\n",
    "P_\\Omega(M) = M_\\Omega\n",
    "$$\n",
    "Lastly, we note that equality constraints could be replaced by normed-difference constraints, of the type $\\|P_\\Omega(X - M)\\|_F \\leq \\eta$, as discussed above.\n",
    "\n",
    "## Nuclear norm minimization\n",
    "\n",
    "### An alternative format\n",
    "\n",
    "We have already seen nuclear norm minimization above. One important point of note is that the nuclear norm minimization problem is equivalent to\n",
    "$$\n",
    "(U^*, V^*) := \\underset{U,V \\in \\mathbb{R}^{n\\times r}}{\\arg\\min} \\,\\, \\| U\\|_F \\|V\\|_F \\quad \\text{subj. to} \\quad \\|P_\\Omega(UV^* - M)\\|_F \\leq \\eta\n",
    "$$\n",
    "when the minimization is performed **simultaneously** over $(U,V)$. Unfortunately, this problem set-up is no longer convex. Nevertheless, efficient iterative algorithms exist for approximating the solution to this problem. In particular, fixing an initial $U^0$, iterate\n",
    "$$\n",
    "\\begin{align*}\n",
    "V^j &:= \\arg\\min \\|U^j\\|_F \\|V\\|_F \\quad \\text{subj. to} \\quad \\|P_\\Omega (U^j V^* - M) \\|_F \\leq \\eta\\\\\n",
    "U^{j+1} &:= \\arg\\min \\|U\\|_F \\|V^j\\|_F \\quad \\text{subj. to} \\quad \\|P_\\Omega (U {V^j}^* - M) \\|_F \\leq \\eta\n",
    "\\end{align*}\n",
    "$$\n",
    "Algorithms that take this iterative approach to approximate solutions have been coined, creatively, alternating-direction methods. It has been shown that the approximation $\\tilde M$ of this particular ADM typically achieves good results, in the sense that \n",
    "$$\n",
    "\\mathrm{rmse}(\\tilde M, M)\n",
    "$$ \n",
    "is \"small\". The advantage that this method has over vanilla nuclear norm minimization is the Frobenius norms now implicitly constrain the rank of $U$ and $V$, meaning that only the noise level $\\eta$ need be controlled. This is important for applications where the rank is not known (the rank is rarely known in advance). We will discuss the problem of unknown rank more, below.\n",
    "\n",
    "## Alternating-direction method (ADM)\n",
    "\n",
    "The first example of an alternating-direction method is actually in the previous section! However, when people think of ADMs for MC, they may typically think of the following problem. \n",
    "\n",
    "### Frobenius norm minimization\n",
    "\n",
    "A simple idea of recovering a low-rank matrix from a set of observations is to minimize the distance between the observed entries, over the set of all low-rank matrices. \n",
    "$$\n",
    "(U,V) := \\underset{U,V \\in \\mathbb{R}^{n\\times r}}{\\arg\\min} \\|P_\\Omega(UV^* - M)\\|_F\n",
    "$$\n",
    "Here, the rank constraint is again provided explicitly; and again, this program is non-convex. Empirical results suggest that the iterative approximate algorithm that *is* convex still achieves good results (in the sense of relative mean-squared error). \n",
    "\n",
    "### Exercises: write an algorithm\n",
    "\n",
    "1. Write an algorithm to solve (or approximately solve) this optimization program.\n",
    "2. *Modify the algorithm to include $\\ell^2$ regularization on the entries of $U$ and $V$. How does this effect the performance of the algorithm?\n",
    "3. Write an algorithm to solve the other form of nuclear norm minimization described above.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "It is important to make a reasonable initial choice for $U^0$; if $U^0$, for example, lies in the space orthogonal to that spanned by the true $U$, then the algorithm implementing the ADM **may never even converge!** For your implementation, let $U^0$ be the top $r$ left singular vectors of $\\frac{1}{p}P_\\Omega(M)$, where $p$ is well-approximated by the quotient of the size of the observations set and the total number of entries of the matrix:  \n",
    "$$ p \\approx \\displaystyle\\frac{\\# M_\\Omega}{mn}$$\n",
    "</div>\n",
    "\n",
    "## Debiasing low-rank projection method\n",
    "\n",
    "### Overview\n",
    "\n",
    "The debiasing low-rank projection method is an empirically verified way of dealing with non-uniformly sampled data. It relies on a tool that is a generalization of the **spectral gap** for a matrix, given by $\\sigma_1 / \\sigma_2$. There is a result from graph theory showing that the spectral gap determines the efficacy of the matrix completion on a given matrix. \n",
    "\n",
    "This is a *new* algorithm, by Foucart, Needell, Plan & Wooters. (So I cannot yet provide the reference). \n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Let $Y \\in \\mathbb{R}^{n\\times n}$ be a fixed and unknown matrix, from which we observe the subset of entries $Y_\\Omega$ for $\\Omega$ a subset of $[n]\\times[n]$. Note that $\\Omega$ is no longer assumed to be random — it can be deterministic!\n",
    "\n",
    "1. Define a low-rank approximation to the masked matrix $Y_\\Omega$: \n",
    "$$\n",
    "\\hat M_0 := \\underset{\\mathrm{rank}(X) \\leq r}{\\arg\\min} \\|X - Y_\\Omega\\|_F\n",
    "$$\n",
    "2. Denote the sampling mask for $\\Omega$ by $\\mathbf{1}_\\Omega$ and let $W$ be the best rank-1 approximation to $\\mathbf{1}_\\Omega$. We will call $W$ the debiasing matrix:\n",
    "$$\n",
    "W := \\underset{\\mathrm{rank}(W) \\leq 1}{\\arg\\min} \\|W - \\mathbf{1}_\\Omega\\|_F\n",
    "$$\n",
    "3. Now define the debiased matrix using $W$, where $X^{(j)}$ corresponds to element-wise exponentiation by $j$ of the matrix $X$; and where $\\circ$ is the Hadamard (element-wise) product. \n",
    "$$\n",
    "\\hat M_{\\mathrm{debias}} := W^{(-1)} \\circ \\hat M_0\n",
    "$$\n",
    "\n",
    "### Results\n",
    "\n",
    "Note that $\\#\\mathrm{measurements} \\geq C (m+n)r \\max(r, \\log(m+n))$ are required to achieve good recovery. Precises error bounds on the approximation are given in the manuscript, but it would require too much background information to describe here. \n",
    "\n",
    "### Exercise: Code the algorithm\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "1. If you feel comfortable using Python, try coding this algorithm! Otherwise, continue following along and come back to coding this algorithm after you have seen implementations of some others, below.  \n",
    "2. **Challenge:** If you choose to implement this algorithm, try it out on the music listening history data set and compare it against one of the other methods described here. There are currently no empirical results directly comparing this algorithm with others!\n",
    "</div>\n",
    "\n",
    "## Max-norm minimization\n",
    "\n",
    "One can also relax the rank constraint in a second way. Namely, via the so-called **max-norm**:\n",
    "$$\n",
    "\\|X \\|_\\text{max} := \\inf_{U,V} \\big\\{\\|U\\|_{2, \\infty} \\|V\\|_{2, \\infty} : X = U V^* \\big\\}\n",
    "$$\n",
    "where, for $A \\in \\mathbb{R}^{m\\times n}$\n",
    "$$\n",
    "\\|A\\|_{2, \\infty} := \\max_{j\\in[m]} \\big( \\sum_{k=1}^n A_{jk}^2 \\big)^{1/2}\n",
    "$$\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "**Note:** there is no satisfying theoretical explanation for why $\\max$-norm minimization works well!\n",
    "</div>\n",
    "\n",
    "### Exercise: write an algorithm\n",
    "\n",
    "1. Write an algorithm that solves (or approximately solves) the max-norm minimization problem. Would you expect this to work better or worse than the methods described above (or does it depend on some properties of the data?)\n",
    "2. Is your method more or less efficient than the other methods described above (*Hint:* how many flops per iteration does your algorithm require? Is it parallelizable?)\n",
    "\n",
    "## On selecting the rank\n",
    "\n",
    "### Issues\n",
    "\n",
    "If the rank is too small, then clearly the recovered matrix $M^*$ cannot be as \"expressive\" as the matrix $M$ — the space it spans is of smaller dimension than that of $M$! This problem will occur regardless of the optimization algorithm we choose. What happens, then, if the rank is too big? In the ideal case, nothing happens: the lowest rank solution is the optimal one and so nothing changes. In reality, this is not the case, particularly with those methods that require an explicit rank to be passed as a parameter. These methods can show disastrous instability if the rank is chosen incorrectly. Moreover, if the rank is too large, then it increases computation time — per iteration cost typically looks like $Crn\\log n$, hence doubling $r$ doubles the time until convergence. \n",
    "\n",
    "### Exercises: making progress on the issues above\n",
    "\n",
    "1. Is there a way to efficiently approximate the rank of a matrix $P_\\Omega(M)$? Can you use this as a way to efficiently find a rank parameter $r$ that gives good recovery on an optimization program that requires a rank parameter $r$? \n",
    "2. For one of the algorithms described above, can you modify the algorithm to better control for instability for incorrectly chosen rank parameters? (**Hint:** is there a regularization that would help?)\n",
    "\n",
    "# Coding matrix completion problems\n",
    "\n",
    "## Nuclear norm minimization for matrix completion\n",
    "\n",
    "### Description\n",
    "\n",
    "In this example, we showcase nuclear norm minimization using the Python package [`cvxpy`](http://www.cvxpy.org/en/latest/index.html). The `cvxpy` package mostly sits on top of [`cvxopt`](http://cvxopt.org/userguide/index.html), which to my knowledge is a port of [`CVX`](http://cvxr.com/cvx/) by Stephen Boyd and Michael Grant. Note that for this example, the default solver will not work, and instead we will use `SCS` &mdash; a splitting conic solver (more details available [here](https://arxiv.org/abs/1312.3039)). \n",
    "\n",
    "### Problem set-up\n",
    "\n",
    "Define the matrix $M$ and the mask $\\Omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = 5\n",
    "m = 100\n",
    "n = 100\n",
    "p = .5\n",
    "k = 1\n",
    "U, V, M, Omega_idx, Omega_mask = matrixCompletionSetup(r, m, n, p, k)\n",
    "M_Omega = masked(M, Omega_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now phrase the optimizaton problem using `cvxpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Variable(*M.shape)\n",
    "Omega_i, Omega_j = matIndicesFromMask(Omega_mask)\n",
    "obj = Minimize(cvxnorm(X, 'nuc') )\n",
    "constraints = [X[Omega_i, Omega_j] == M_Omega]\n",
    "prob = Problem(obj, constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob.solve(solver=SCS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_comparison(M, X.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement an alternating-direction method\n",
    "\n",
    "### Exercises: Implement the ADM algorithm for Frobenius norm minimization\n",
    "\n",
    "1. Use `cvxpy` to implement the ADM for Frobenius norm minimization with rank $r$. \n",
    "2. Bonus: re-formulate the ADM using the matrix re-shaping above, and write a new algorithm that approximately solves the Frobenius norm minimization using least squares. (see Aaron for helpful code for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a small problem\n",
    "m = n = 50\n",
    "r = 3\n",
    "p = .5\n",
    "k = 1\n",
    "U, V, M, Omega_idx, Omega_mask = matrixCompletionSetup(r, m, n, p, k)\n",
    "M_Omega = masked(M, Omega_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`altMinSense` implementation using [`cvxpy`](http://www.cvxpy.org/) and [Jain & Netrapalli, 2012](https://arxiv.org/pdf/1212.0467.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U_cvx, V_cvx = altMinSense(M_Omega=M_Omega, Omega_mask=Omega_mask, r=2, method='cvx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_comparison(M, U_cvx @ V_cvx.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info alert-block'>\n",
    "We need $\\mathcal{O}(rn \\log n)$ samples of the matrix in order to guarantee good recovery; when the matrix is this small, that's very hard to do (because the constant hidden by the $\\mathcal{O}$ could be large). If we want a better demonstration of the recovery method, then we need to start with bigger data. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = 5\n",
    "m = 100\n",
    "n = 100\n",
    "p = .5\n",
    "k = 1\n",
    "U, V, M, Omega_idx, Omega_mask = matrixCompletionSetup(r, m, n, p, k)\n",
    "M_Omega = masked(M, Omega_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U_approx, V_approx = altMinSense(M_Omega=M_Omega, Omega_mask=Omega_mask, r=r)\n",
    "M_opt = U_approx @ V_approx.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_comparison(M, M_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debiasing low-rank projection implementation\n",
    "\n",
    "### Approximate observation mask\n",
    "\n",
    "**From graph theory:** we can view the observation mask as an adjacency matrix of a $d$-regular graph; it is guaranteed that the leading left singular vector of this graph has positive entries. \n",
    "\n",
    "**From linear algebra:** the leading singular vectors of a matrix yield the best rank-1 approximation to that matrix. \n",
    "\n",
    "**From MC and graph theory:** the best rank-1 approximation to the observation mask gives a generalization of the spectral gap that guarantees recovery of a matrix. \n",
    "\n",
    "The computation of the leading singular vectors will be fastest if we have a sparse observation mask. We have already imported `svds`, the sparse svd solver from `scipy.sparse.linalg`. \n",
    "\n",
    "#### Method 1: `svds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparseMask = csr_matrix(Omega_mask.astype(np.int)).asfptype()\n",
    "u, s, vt = svds(sparseMask, k=1)\n",
    "W = (s*u) @ vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_comparison(Omega_mask, W, cmap='gray',\n",
    "                plot_titles=['$\\Omega$', '$W$'], \n",
    "                error_title='absolute error $|\\Omega - W|$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: `altMinSense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_l, W_r = altMinSense(vec(Omega_mask), np.ones(Omega_mask.shape), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = W_l @ W_r.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_comparison(Omega_mask, W2, cmap='gray',\n",
    "                plot_titles=['$\\Omega$', '$W$'], \n",
    "                error_title='absolute error $|\\Omega - W|$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_comparison(W, W2, cmap='gray',\n",
    "                plot_titles=['$W_{\\mathrm{svds}}$', '$W_{\\mathrm{altMinSense}}$'], \n",
    "                error_title='absolute error $|W_{\\mathrm{svds}} - W_{\\mathrm{altMinSense}}|$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This achieves the same thing, but it actually takes longer! Also, there's a trick as to why it recovers the same thing (since the same thing would not work in this way for what we're about to do next...)\n",
    "\n",
    "### Compute $\\hat M_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_masked = csr_matrix(M*Omega_mask).asfptype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u_masked, s_masked, vt_masked = svds(M_masked, k=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M0 = (u_masked * s_masked) @ vt_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_comparison(M*Omega_mask, M0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the Hadamard product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_debias = M0 * W**(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_comparison(M, M_debias, cmap='viridis',\n",
    "                plot_titles=['$M$', '$\\hat M_\\mathrm{debias}$'], \n",
    "                error_title='$|M - \\hat M_\\mathrm{debias}|$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't working too well! Not sure why...\n",
    "\n",
    "# On the use of sparse matrices\n",
    "\n",
    "**Disclosure:** I didn't sparsify everything as much as I could have. But one should do this when one has many, many zero-valued entries in high-dimensional matrices. A justification for this follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sizeOfCsr(A):\n",
    "    \"\"\"\n",
    "    sizeOfCsr(A) returns size (in bytes) of\n",
    "        the compressed sparse row matrix A.\n",
    "    \"\"\"\n",
    "    return A.data.nbytes + A.indptr.nbytes + A.indices.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compressionFactor(A, A_csr):\n",
    "    return sizeOfCsr(A_csr) / A.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "m = 1000\n",
    "n = 10000\n",
    "delta = .1 # sparsity proportion\n",
    "s = np.int(delta * n) # sparsity level\n",
    "A_nnz = np.random.randn(s)\n",
    "A_ij = (np.random.randint(m, size=s), np.random.randint(n, size=s))\n",
    "A = np.zeros((m,n))\n",
    "A[A_ij] = A_nnz\n",
    "A_sparse = csr_matrix(A)\n",
    "print('Compression factor = {} %'.format(np.round(compressionFactor(A, A_sparse)*100, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The realm of parameter guessing\n",
    "\n",
    "What happens when we have to guess the rank? Things no longer work so well!\n",
    "\n",
    "## What happens when rank is chosen incorrectly?\n",
    "\n",
    "In practice we **\"never\"** know the rank. Sometimes, we're happy to correctly under-estimate the rank (we think it's lower than it really is), because perhaps we want a lower-dimensional approximation of what's going on. However, there are other times where it is important to accurately determine the rank, and in these cases we do not want to over-estimate! Why?\n",
    "* Choosing the rank too large means that our algorithms (which run in $\\mathcal{O}(r \\log n)$) take longer to solve\n",
    "* What if choosing the rank too large causes \n",
    "   * overfitting?\n",
    "   * instability?\n",
    "   * huge errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U_approx, V_approx = altMinSense(M_Omega, Omega_mask=Omega_mask, r=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_comparison(M, U_approx @ V_approx.T, \n",
    "                plot_titles=['$M$', '$\\\\hat M := \\\\hat U \\\\hat V^T$'], \n",
    "                error_title='$|\\hat M - M|$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens in the presence of noise?\n",
    "\n",
    "How does this modify the algorithm? Can we gain any advantages? \n",
    "\n",
    "# Out-of-the-box packages\n",
    "\n",
    "Below are some packages for out-of-the-box implementations of algorithms related to matrix completion (*e.g.* nuclear norm minimization, matrix least squares, *etc.*).\n",
    "\n",
    "* `scipy` : Iterative SVD; Sparse Methods; Least Squares Solvers\n",
    "* `sklearn` : Nonnegative Matrix Factorization; Truncated SVD\n",
    "* `pyspark.mllib` : Distributed and parallel implementation; k-Nearest Neighbours\n",
    "* `fancyimpute` : Nuclear norm minimization; Bayesian ridge regression; SoftImpute; Similarity-weighted averages; *etc.*\n",
    "* `keras` and `tensorflow` : Powerful ML methods for learning low-dimensional sub-spaces\n",
    "\n",
    "# Example: Seismic Data\n",
    "\n",
    "Our first real example will be trying to recover a slice of seismic data, given highly incomplete measurements. We can find out more about this data when we load it in to the workspace. For this purpose, the `mc_util` module has a `load_seismic_data` function.\n",
    "\n",
    "A special thanks to [Oscar Lopez](https://www.slim.eos.ubc.ca/content/oscar-lopez-0), who put together the data and the write-up for us (and spent some time filling me in on some important details)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seismic = load_seismic_data(directory='./data/seismic/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises: visualizing the input data\n",
    "\n",
    "In essence, we have a single frequency slice of a 5D tensor, meaning that we have a 4D tensor, which has in turn been flattened into a matrix. Why should we expect this matrix to have small rank? Let's visualize it to find out! Create a function, `plot_cplx`, that takes a complex-valued input matrix and plots the real and imaginary components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_cplx(seismic, cmap='gray', cb_tied=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Setting up the problem\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "A small note: `cvxpy` doesn't work out of the box with complex-valued data. `cvxopt` is the package that `cvxpy` sits on top of; it does support complex-valued data but is much less intuitive to use (e.g. we can't use the same problem formulation as we used above; we must first set it up as a semi-definite program, for example, before solving. `picos` is another Python package for semi-definite programming that supports complex-valued variables. \n",
    "\n",
    "In order to keep this tutorial consistent, we will stick with the easy-to-use `cvxpy` package and recover the **modulus** of the seismic data. Savvy tutorialers can take this a step further and use the amplitude information [and complex-valued observations] to then solve a **phase retrieval** problem. \n",
    "</div>\n",
    "\n",
    "For this example, we'll be trying to recover the **modulus** of the seismic data from a ~50% undersampling of the data. Write a function, `seismic_mask`, that:\n",
    "1. Computes a uniform-at-random sampling of observed values of the seismic array\n",
    "2. Returns `maskArr`, a Boolean (or 0-1 valued) array whose True (resp. 1) entries mark those indices that were observed\n",
    "3. Returns `maskIdx`, a tuple containing a list of the row-indices and a list of the column-indices that mark those indices of the seismic data that were observed. \n",
    "4. Returns `seismicObs`, a **vector** of the observed seismic entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maskArr, maskIdx, seismicObs = seismic_mask(seismic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the seismic observations\n",
    "\n",
    "For illustration purposes, it would be really nice to see which data we have observed and what our observation pattern looks like. To do this, we could plot `seismic * maskArr`, but this will muddle the zeros in the mask with the near-0 values in the data. To *really* see what it looks like, we'll instead plot `seismic / maskArr` so that the observed value remain unchanged, but the unobserved entries are now all `inf` (equally, we could have set these to `nan`, *etc.*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_cplx(seismic / maskArr, cmap='gray', cb_tied=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we'll actually be performing the recovery on the modulus of the array.\n",
    "\n",
    "### Exercise: Recover the array\n",
    "\n",
    "Use the `altMinSense` algorithm that we developed above to recover the modulus of the seismic array from the 50% undersampled measurements. \n",
    "\n",
    "**Problems**\n",
    "1. How do you choose the rank `r`?\n",
    "2. What is the best choice for the rank?\n",
    "\n",
    "**Bonus Problems**\n",
    "1. Use `cvxpy` to implement the ADM for nuclear norm minimization. \n",
    "2. How do choices for the rank affect the error and optimization time for this algorithm? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seisU_lowRank, seisV_lowRank = altMinSense(np.abs(seismicObs), maskArr, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective value is $2.3$ in this example. Now what is the error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Relative mean-squared error = {}'.format(rmse(seisU_lowRank @ seisV_lowRank.T, np.abs(seismic))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seisU, seisV = altMinSense(np.abs(seismicObs), maskArr, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective value is 0.42 — does this mean that we did *better* by increasing the rank? Let's verify by looking at the error between the data and the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Relative mean-squared error = {}'.format(rmse(seisU @ seisV.T, np.abs(seismic))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have worse RMSE by using too high a rank. And it took way more time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Movie ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this tutorial we will use the [Movie Lens](https://grouplens.org/datasets/movielens/) dataset. Full details on the specific data set that we will be using are available in a [readme for the data](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html). The [data can be downloaded](https://grouplens.org/datasets/movielens/) from the homepage, but for the purposes of this tutorial, we'll all use the data stored in the folder that lies your home directory.\n",
    "\n",
    "\n",
    "### About the data\n",
    "\n",
    "From the website:\n",
    "\n",
    "> This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100004 ratings and 1296 tag applications across 9125 movies. These data were created by 671 users between January 09, 1995 and October 16, 2016. This dataset was generated on October 17, 2016.\n",
    "\n",
    "> Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "> The data are contained in the files links.csv, movies.csv, ratings.csv and tags.csv. More details about the contents and use of all these files follows.\n",
    "\n",
    "> This is a development dataset. As such, it may change over time and is not an appropriate dataset for shared research results. See available benchmark datasets if that is your intent.\n",
    "\n",
    "> This and other GroupLens data sets are publicly available for download at http://grouplens.org/datasets/.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links, movies, ratings, tags = load_movie_ratings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Complete the movies!\n",
    "\n",
    "1. Construct a *mask* matrix from the `ratings` variable. Use a matrix completion method described above to approximate a solution to the matrix completion problem. How will you choose your rank, `r`?\n",
    "2. What other interesting things could you find from the data? (`tags` looks like it could be fun.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Music preference data\n",
    "\n",
    "The taste music data set and a 1% chunk of the million song data set are available in the `~/data` for you to import and play with. To untar a tarball, `tar -xvzf my_file.tar.gz`. There's a chance that the machine won't allow us to unzip. \n",
    "\n",
    "[See here](https://labrosa.ee.columbia.edu/millionsong/) for information on and downloads for the million song data set.\n",
    "\n",
    "\n",
    "# Further Reading\n",
    "\n",
    "For more information on matrix completion, see these papers:\n",
    "\n",
    "1. [Exact matrix completion via convex optimization](https://statweb.stanford.edu/~candes/papers/MatrixCompletion.pdf) is a flagship paper by Recht, *et al.*\n",
    "2. [Low-rank matrix completion using alternating-minimization](https://arxiv.org/pdf/1212.0467.pdf) is the original publication containing the `altMinSense` algorithm and other variations. \n",
    "2. [Beating level-set methods for 5-D seismic data interpolation: a primal-dual alternating approach](http://ieeexplore.ieee.org/abstract/document/7906537/) uses a new method for matrix completion for applications in geophysical inverse problems\n",
    "4. [Matrix completion via max-norm constrained optimization](http://www-stat.wharton.upenn.edu/~tcai/paper/Max-Norm-Matrix-Completion.pdf) details methods for max-norm based matrix completion, with a good amount of background material\n",
    "5. [Practical large-scale optimization for max-norm regularization](https://people.eecs.berkeley.edu/~brecht/papers/maxnorm.NIPS10.pdf) details methods for max-norm based matrix completion\n",
    "6. [Yaniv's course notes](http://www.yanivplan.com/files/Lecture-19.jnt_.pdf) cover matrix completion in an abstract setting\n",
    "7. [Matrix completion on graphs](https://github.com/victorjourne/Matrix-completion-over-graphs/blob/master/Matrix%20completion%20on%20graphs.ipynb) is Jupyter notebook with a neat application to social network that uses a generalization of MC to graphs."
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/8c8fd1e7f384df0183b8f01423c07a55"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "mini-project-solutions/2-mc-solutions/matrix_completion_master.ipynb",
    "public": true
   },
   "id": "8c8fd1e7f384df0183b8f01423c07a55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "618px",
    "left": "0px",
    "right": "1227px",
    "top": "106px",
    "width": "202px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
